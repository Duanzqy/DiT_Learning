## Paper
- [ ] [Hallo4](https://arxiv.org/abs/2505.23525)
- [ ] [Wan-Animate](https://humanaigc.github.io/wan-animate/)
- [ ] [DPoser-X](https://github.com/moonbow721/DPoser-X )
- [ ] [klingavatar](https://klingavatar.github.io/)

## 0922

- [x] CAGD HW1: 函数拟合
- [x] FEM: HW1
### Hallo4



## 0923
### CAGD：Bezier Curve

#### Curve Presentation
1. Implicit curve $f(x,y)=0$
	- 相同 x 多个值
	- 存在导数不存在点（无穷大）
2. parametric presentation $c(t)=(x(t), y(t))$
	- 容易估计
	- t 可以解释为时间

#### Model with power basis:
抛物线(Parabola): $f(t) = a t^2 + b t + c$ ,如：
![](attachment/Pasted%20image%2020250923170750.png)
这缺乏直观的几何意义

---
但是我们从参数化的直线出发，如下图所示，得到抛物线的另一种表示，就具有直观的几何意义
![](attachment/Pasted%20image%2020250923170935.png)
因此，我们将抛物线换一种表示方式，如下，这些系数就有明显的几何意义
![](attachment/Pasted%20image%2020250923171222.png)
更近一步，我们可以添加更多的控制点，进一步迭代，就能得到更高阶的 Bezier 曲线
#### De Casteljau algorithm
简要描述：
- 选取 $radio = t \in (0,1)$ 
- 对给定 $n$ 个点，连接相邻两点并取按比例 t 插值得到 $n-1$ 个新点
- 对所有新得到的点，按相同比例 t 重复上一步操作
- 迭代 $n-1$ 次，直到剩下一个点
如下图所示
![](attachment/Pasted%20image%2020250923171907.png)

``` Algorithm
for r in (1, n)
	for i in (0, n-r)
		b[r][i] = (1-t)*b[r-1][i] + t * b[r-1][i+1]
	end
end
return b[n][0]
```

#### Properties of Bezier Curve 
Given Bezier points: $b_0, b_1, ..., b_n$ , Bezier curve $x(t)$
- deg(x(t)) = n
- $x(0)= b_0, x(1)=b_n$
- **convex hull property**: $x(t)$ 完全在 Bezier points 形成的凸多边形中
- influence of bezier point:
	- global: 改变任意 bezier point 都会影响曲线形状
	- Pseudo-local: 改变 $b_i$ 对 $x(i/n)$ 影响最大
- 仿射变换不变性
- 对称性

---
以上我们从几何直观得到了 Bezier Curve，由表达式我们得到了它的性质，现在考虑从性质出发，从代数方法能否得到一样的曲线？（即反方向）
#### with an algebraic approach using basis functions
###### Problem：
- Given $n+1$ points $b_0,b_1, ..., b_n$
- Target: Bezier Curve $x(t) = \sum_{i=0}^{n} B_i^n(t) b_i$

###### Desirable Properties
- Smoothness
- Local control / support
- Affine invariance
- Convex hull property

##### Bernstein Basis
- Bernstein basis: $B = \{B_0^{(n)}, B_1^{(n)},..., B_n{(n)}\}$
- $B_i^{(n)}= \binom{n}{i} t^i (1-t)^{n-i}= B_{i-th\,basis\,function}^{degree}$

### Wan-Animate
[reading note](paper/Wan-Animate.md)

## 0924

突然想到一个问题：为什么现在视觉大模型训练中，好像没有利用图像的梯度等信息作为额外信息注入？像传统图形学中 possion 融合那样，利用梯度表示连续性？

可能的原因：
- 梯度只是一种函数，且可以显示表示出来，现在都用隐式的神经网络来表示函数，利用网络可以表示出各种辅助额外信息，例如身体关键点、smpl等等，这些比梯度含有更具体的信息
- 神经网络是让计算机通过大量数据学习出来的函数，而利用梯度等显示函数来表示额外信息，相当于人类通过经验、推理等途径得出的函数