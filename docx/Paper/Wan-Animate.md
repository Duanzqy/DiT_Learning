![](../attachment/Pasted%20image%2020250923220324.png)

### 解决的问题
Input：角色图像 + 参考视频
output：两种方式
- 角色动画：源图像中的角色会根据参考视频中角色的动作进行动画处理，同时保留源图像中的背景
- 替换：源图像中的角色由相同的参考动作驱动，但随后被整合到参考视频的环境中。将角色替换掉参考视频中的人物，保持视频中的环境、光照

### 方法论概括
1. 基于Wan-I2V模型，因此需要针对角色动画需求而修改的输入定义
2. 添加身体骨骼与脸部表情两种控制条件，按两种不同方式注入，其中关注如何解决身体的脸部与脸部表情注入的冲突
3. Relighting LoRA 增强环境光照与色调


### 具体方法
##### 1. 输入适配模型
输入包括：noise latent, conditional latent, binary mask

binary mask(二值掩码) 与 conditional latent具有相同时空维度，1表示保留的帧，0表示要生成的帧
- 与 Wan-I2V 不同：
	- I2V 以 img 作为起始帧，binary mask设为1，但 Animate 要求角色图像作为一致的外观参考，生成视频的内容由驱动信号决定
	- 动画化生成长视频，后续片段需要将前一段视频的后几帧条件化（难道I2V不需要长视频生成吗？）
	- 要兼容动画与替换两种模型，减少额外训练
- 基于上述原因进行输入的修改：
	- 给的参考角色图像，Wan-Animate将其编码为专用的 ref latent
	- ref latent (binary mask 设为1)与 conditional latent 按时间维度进行拼接
	- 从目标序列（指的是什么？）随机选取前几个latent 作为 tempo latent（时间潜在表示），选取的latent中 真实值作为 conditional latent，binary mask在整个帧设为1
	- 概率性训练策略：时间潜在表示仅以一定概率使用
- 环境生成方法：
	- 动画模型：参考视频帧的条件帧 binary mask 设为0，在保留给定参考图像背景的同时生成角色视频，类似于 I2V 任务
	- 替换模式：从参考视频分割角色，**遵循 Hu 等人（2025）提出的掩码建模策略**（需要看一下），通过清0的主体区域生成环境图像作为条件帧，环境区域设为1，主体区域设为0，Wan-Animate仅生成 0 中的内容

##### 2. 控制信号
###### 身体控制
对比两种控制信号：
- 2d 骨骼关键点：
	- 泛用性高，适合非人类角色
	- 难以处理复杂运动，由关键点缺失问题
- 3D SMPL：
	- 准确表示肢体关系，但肢体末端精度不高，对非人体表达较差



###### 面部控制
![](../attachment/Pasted%20image%2020250924093834.png)
面部关键点问题：
- 损失细节信息
- 跨身份场景中面部形变严重，精度不够
因此采用直接输入原始脸部图像作为驱动，利用骨骼信息从ref img中裁剪脸部区域

因为训练是自监督的，所有需要在提取面部特征时，将身份与表情信息分离：
- 压缩为1D latent
- 对面部图像采用多种数据增强

仅在 DiT 特定层进行注入


##### 3. Relighting LoRA
分割视频采样图像的角色，使用 IC-Light 合成到新的随机背景（不同的光照、色调），得到新的图像作为参考图像，用于训练


#### 训练、推理细节
1. 身体控制训练：仅使用身体信号，让模型快速学习修改后的输入范式
2. 面部控制训练：这里使用面部关键点来识别头部、眼睛和嘴部区域，对这些区域应用更高的损失权重以增强其保真度
3. 推理中的姿态重定向
	1. 计算骨骼长度的缩放因子：使用图像编辑模型将参考图像与驱动视频的姿势变为T-Pose
	2. 将驱动姿态的骨骼调整为与参考图像匹配
	3. 平移姿态与参考图像的角色对齐
